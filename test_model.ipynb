{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install descript-audio-codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import os\n",
    "import torch\n",
    "import torchaudio.functional as F\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from audiotools import AudioSignal\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class AudioData:\n",
    "    def __init__(self, audio_data, sample_rate):\n",
    "        self.audio_data = audio_data\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def to(self, device):\n",
    "        self.audio_data = self.audio_data.to(device)\n",
    "        self.sample_rate = torch.tensor(self.sample_rate, device=device)\n",
    "        return self\n",
    "\n",
    "    def device(self):\n",
    "        return self.audio_data.device\n",
    "\n",
    "    def clone(self):\n",
    "        return AudioData(self.audio_data.clone(), self.sample_rate)\n",
    "\n",
    "    def audio_data(self):\n",
    "        return self.audio_data\n",
    "\n",
    "    def sample_rate(self):\n",
    "        return self.sample_rate\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths_sound, file_paths_noise, target_sample_rate=44100):\n",
    "        self.file_paths_sound = file_paths_sound\n",
    "        self.file_paths_noise = file_paths_noise\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.fixed_length = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths_sound)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.file_paths_sound[idx]\n",
    "        noise_path = np.random.choice(self.file_paths_noise)\n",
    "\n",
    "        audio_data, sample_rate = torchaudio.load(audio_path)\n",
    "        if sample_rate != self.target_sample_rate:\n",
    "            resample_transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sample_rate)\n",
    "            audio_data = resample_transform(audio_data)\n",
    "        signal = AudioData(audio_data, sample_rate)\n",
    "\n",
    "        audio_data_noise, sample_rate_noise = torchaudio.load(noise_path)\n",
    "        if sample_rate_noise != self.target_sample_rate:\n",
    "            resample_transform = torchaudio.transforms.Resample(orig_freq=sample_rate_noise, new_freq=self.target_sample_rate)\n",
    "            audio_data_noise = resample_transform(audio_data_noise)\n",
    "        noise = AudioData(audio_data_noise, sample_rate_noise)\n",
    "\n",
    "        # Convert to mono if stereo\n",
    "        if signal.audio_data.shape[0] > 1:\n",
    "            signal.audio_data = torch.mean(signal.audio_data, dim = -2, keepdim = True)\n",
    "        if noise.audio_data.shape[0] > 1:\n",
    "            noise.audio_data = torch.mean(noise.audio_data, dim = -2, keepdim = True)\n",
    "\n",
    "        # Pad or trim to the specified length of 5 seconds\n",
    "        self.fixed_length = int(signal.sample_rate*4)\n",
    "        current_length = signal.audio_data.shape[1]\n",
    "        if current_length < self.fixed_length:\n",
    "            # Pad if the signal is shorter than the fixed length\n",
    "            padding = self.fixed_length - current_length\n",
    "            signal.audio_data = torch.nn.functional.pad(signal.audio_data, (0, padding))\n",
    "        elif current_length > self.fixed_length:\n",
    "            start_position = np.random.randint(0, max(1, signal.audio_data.shape[1] - self.fixed_length))\n",
    "            signal.audio_data = signal.audio_data[:, start_position:start_position + self.fixed_length]\n",
    "\n",
    "        self.fixed_length = int(noise.sample_rate*4)\n",
    "        current_length = noise.audio_data.shape[1]\n",
    "        if current_length < self.fixed_length:\n",
    "            padding = self.fixed_length - current_length\n",
    "            noise.audio_data = torch.nn.functional.pad(noise.audio_data, (0, padding))\n",
    "        elif current_length > self.fixed_length:\n",
    "            start_position = np.random.randint(0, max(1, noise.audio_data.shape[1] - self.fixed_length))\n",
    "            noise.audio_data = noise.audio_data[:, start_position:start_position + self.fixed_length]\n",
    "\n",
    "        return signal, noise\n",
    "\n",
    "def collate_fn(batch):\n",
    "    speech, noise = zip(*batch)\n",
    "    speech = [signal.audio_data for signal in speech]\n",
    "    noise = [signal.audio_data for signal in noise]\n",
    "\n",
    "    return torch.stack(speech), torch.stack(noise)\n",
    "\n",
    "# Specify the file paths as before\n",
    "drive_path = \"\" #If path to other folders is needed\n",
    "audio_folder = 'Sound_Data/Data'\n",
    "noise_folder = \"Sound_Data/Noise\"\n",
    "file_paths_sound = [os.path.join(drive_path, audio_folder, filename) for filename in os.listdir(os.path.join(drive_path, audio_folder))]\n",
    "file_paths_noise = [os.path.join(drive_path, noise_folder, filename) for filename in os.listdir(os.path.join(drive_path, noise_folder))]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_paths_sound, test_paths_sound = train_test_split(\n",
    "    file_paths_sound, test_size=0.2, random_state=42\n",
    ")\n",
    "train_paths_noise, test_paths_noise = train_test_split(\n",
    "    file_paths_noise, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets and dataloaders for training and testing\n",
    "train_dataset = AudioDataset(train_paths_sound, train_paths_noise)\n",
    "test_dataset = AudioDataset(test_paths_sound, test_paths_noise)\n",
    "\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics.audio import ScaleInvariantSignalNoiseRatio, SignalNoiseRatio\n",
    "\n",
    "class SNRLoss(nn.Module):\n",
    "    def forward(self, target, predicted):\n",
    "        # SNR\n",
    "        snr = SignalNoiseRatio().to(device)\n",
    "        snr_loss = snr(predicted, target)\n",
    "\n",
    "        # Return the negative SNR (as it is a loss, higher SNR is better)\n",
    "        return -snr_loss\n",
    "\n",
    "class SI_SNRloss(nn.Module):\n",
    "    def forward(self, target, predicted):\n",
    "        # SI-SNR\n",
    "        si_snr = ScaleInvariantSignalNoiseRatio().to(device)\n",
    "        si_snr_loss = si_snr(predicted, target)\n",
    "\n",
    "        # Return the negative SNR (as it is a loss, higher SNR is better)\n",
    "        return -si_snr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import dac\n",
    "import torchaudio.functional as F\n",
    "import torchaudio\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "# Create the model, loss function, and optimizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "target_sample_rate = 44100  # Replace with your desired sample rate\n",
    "\n",
    "# Download a model\n",
    "model_path = dac.utils.download(model_type=\"44khz\")\n",
    "model = dac.DAC.load(model_path)\n",
    "\n",
    "model = model.to(device).train()\n",
    "criterion = nn.L1Loss() # NOTE Change loss function \n",
    "lr = 1e-7\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1)\n",
    "\n",
    "train_loss_vec = []\n",
    "test_loss_vec = []\n",
    "epoch_vec = []\n",
    "snr = torch.tensor([[2]]).cuda()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for signal, noise in train_dataloader:\n",
    "        signal = signal.cuda()\n",
    "        noise = noise.cuda()\n",
    "        # Create noisy signal\n",
    "        noise += 1e-5\n",
    "        noisy_signal = F.add_noise(signal, noise, snr)\n",
    "        noise = None\n",
    "\n",
    "        \"\"\"\n",
    "        # Normalize Signals\n",
    "        signal_audio_data = signal.audio_data/torch.max(torch.abs(signal.audio_data))\n",
    "        noisy_signal.audio_data = noisy_signal.audio_data/torch.max(torch.abs(noisy_signal.audio_data))\n",
    "        \"\"\"\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        noisy_signal.to(model.device)\n",
    "        #noisy_signal = torch.unsqueeze(noisy_signal,dim=1)\n",
    "        x = model.preprocess(noisy_signal, 44100)\n",
    "        noisy_signal = None\n",
    "        z, codes, latents, _, _ = model.encode(x)\n",
    "        x = None\n",
    "\n",
    "        # Decode audio signal\n",
    "        y = model.decode(z)\n",
    "        z = None\n",
    "\n",
    "        if y.shape[2] > signal.shape[2]:\n",
    "            y = y[:, :, :signal.shape[2]]\n",
    "        elif y.shape[2] < signal.shape[2]:\n",
    "            padding = signal.shape[2] - y.shape[2]\n",
    "            y = torch.nn.functional.pad(y, (0, padding))\n",
    "\n",
    "        # Calculate the loss for the current signal\n",
    "        loss = criterion(y, signal)\n",
    "        y = None\n",
    "        signal = None\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print training statistics\n",
    "    train_loss_vec.append(total_loss / len(train_dataloader))\n",
    "    epoch_vec.append(epoch)\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {total_loss / len(train_dataloader)}')\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    total_test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for signal, noise in test_dataloader:\n",
    "            signal = signal.cuda()\n",
    "            noise = noise.cuda()\n",
    "            # Create noisy signal\n",
    "            noise += 1e-5\n",
    "            noisy_signal = F.add_noise(signal, noise, snr)\n",
    "\n",
    "            \"\"\"\n",
    "            # Normalize Signals\n",
    "            signal_audio_data = signal.audio_data/torch.max(torch.abs(signal.audio_data))\n",
    "            noisy_signal.audio_data = noisy_signal.audio_data/torch.max(torch.abs(noisy_signal.audio_data))\n",
    "            \"\"\"\n",
    "            # Forward pass\n",
    "            noisy_signal = noisy_signal.to(model.device)\n",
    "            x = model.preprocess(noisy_signal, 44100)\n",
    "            z, codes, latents, _, _ = model.encode(x)\n",
    "\n",
    "            # Decode audio signal\n",
    "            y = model.decode(z)\n",
    "\n",
    "            if y.shape[2] > signal.shape[2]:\n",
    "                y = y[:, :, :signal.shape[2]]\n",
    "            elif y.shape[2] < signal.shape[2]:\n",
    "                padding = signal.shape[2] - y.shape[2]\n",
    "                y = torch.nn.functional.pad(y, (0, padding))\n",
    "\n",
    "            # Calculate the loss for the current signal\n",
    "            loss = criterion(y, signal)\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "    # Calculate average test loss\n",
    "    average_test_loss = total_test_loss / len(test_dataloader)\n",
    "    test_loss_vec.append(average_test_loss)\n",
    "    # Print testing statistics\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Test Loss: {average_test_loss}')\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), f'trained_L1_model_{epoch}.pth')\n",
    "\n",
    "# Save Loss data\n",
    "np.savetxt(f\"L1TestLoss_{epoch}\", test_loss_vec)\n",
    "np.savetxt(f\"L1TrainLoss_{epoch}\", train_loss_vec)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
